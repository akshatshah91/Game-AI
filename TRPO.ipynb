{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trpo_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PxMV_52NEf8-",
        "yIApAqWAEf-_",
        "9DpoxxPzPpOV",
        "7QVvR6xCPwL3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatshah91/Game-AI/blob/master/TRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4L_qrz3EctY"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iY_BMrdGdzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb9a1bc-a24c-45b9-aa82-cb09655d12d2"
      },
      "source": [
        "!pip install procgen\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import scipy.optimize\n",
        "import math\n",
        "import random\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: procgen in /usr/local/lib/python3.6/dist-packages (0.10.4)\n",
            "Requirement already satisfied: gym3<1.0.0,>=0.3.3 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (1.18.5)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (3.0.12)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.17.3)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.5.4 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (5.6.2)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.9.0)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.12.0)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.14.4)\n",
            "Requirement already satisfied: imageio-ffmpeg<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (0.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n",
            "Requirement already satisfied: glcontext<3,>=2 in /usr/local/lib/python3.6/dist-packages (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen) (2.2.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (7.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxMV_52NEf8-"
      },
      "source": [
        "### Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXMICCN9Iuzs"
      },
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'mask', 'next_state',\n",
        "                                       'reward'))\n",
        "class Memory:\n",
        "  def __init__(self):\n",
        "    self.memory = []\n",
        "\n",
        "  def push(self, *args):\n",
        "    self.memory.append(Transition(*args))\n",
        "  \n",
        "  def sample(self):\n",
        "    return Transition(*zip(*self.memory))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIApAqWAEf-_"
      },
      "source": [
        "#Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P6U3sbBHknW"
      },
      "source": [
        "class Policy(nn.Module):\n",
        "  def __init__(self, num_inputs, num_outputs):\n",
        "    super(Policy, self).__init__()\n",
        "\n",
        "    self.fc_layers = nn.Sequential(\n",
        "        nn.Linear(num_inputs, 64),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "    self.action_mean = nn.Linear(64, num_outputs)\n",
        "    self.action_mean.weight.data.mul_(0.1)\n",
        "    self.action_mean.bias.data.mul_(0.0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc_layers(x)\n",
        "\n",
        "    action_prob = torch.softmax(self.action_mean(x), dim=1)\n",
        "\n",
        "    return action_prob\n",
        "\n",
        "  def select_action(self, x):\n",
        "    x = Variable(torch.from_numpy(x).unsqueeze(0)).cuda()\n",
        "    action_prob = self.forward(x)\n",
        "    action = action_prob.multinomial(1)\n",
        "    return action\n",
        "\n",
        "  def get_kl(self, x):\n",
        "    action_prob1 = self.forward(x)\n",
        "    action_prob0 = action_prob1.detach()\n",
        "    kl = action_prob0 * (torch.log(action_prob0) - torch.log(action_prob1))\n",
        "    return kl.sum(1, keepdim=True)\n",
        "  \n",
        "  def get_log_prob(self, x, actions):\n",
        "    action_prob = self.forward(x)\n",
        "    # print(action_prob.shape)\n",
        "    # print(actions.shape)\n",
        "    # print(actions.long().shape)\n",
        "    # print(actions.long().unsqueeze(1).shape)\n",
        "    \n",
        "    return torch.log(action_prob.gather(1, actions.long().unsqueeze(1)))\n",
        "  \n",
        "  def get_fim(self, x):\n",
        "    action_prob = self.forward(x)\n",
        "    M = action_prob.pow(-1).view(-1).detach()\n",
        "    return M, action_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Nbq4HMIgY-"
      },
      "source": [
        "class Value(nn.Module):\n",
        "  def __init__(self, num_inputs):\n",
        "    super(Value, self).__init__()\n",
        "    self.fc_layers = nn.Sequential(\n",
        "        nn.Linear(num_inputs, 64),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    self.value_head = nn.Linear(64, 1)\n",
        "    self.value_head.weight.data.mul_(0.1)\n",
        "    self.value_head.bias.data.mul_(0.0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc_layers(x)\n",
        "\n",
        "    state_values = self.value_head(x)\n",
        "    return state_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYgccn2AO1Sr"
      },
      "source": [
        "def estimate_advantages(rewards, masks, values, gamma, tau, device):\n",
        "  rewards, masks, values = to_device(torch.device('cpu'), rewards, masks, values)\n",
        "  tensor_type = type(rewards)\n",
        "  deltas = tensor_type(rewards.size(0), 1)\n",
        "  advantages = tensor_type(rewards.size(0), 1)\n",
        "\n",
        "  prev_value = 0\n",
        "  prev_advantage = 0\n",
        "  for i in reversed(range(rewards.size(0))):\n",
        "    deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values[i]\n",
        "    advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n",
        "\n",
        "    prev_value = values[i, 0]\n",
        "    prev_advantage = advantages[i, 0]\n",
        "\n",
        "  returns = values + advantages\n",
        "  advantages = (advantages - advantages.mean()) / advantages.std()\n",
        "\n",
        "  advantages, returns = to_device(device, advantages, returns)\n",
        "  return advantages, returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKhPB0TpEgFA"
      },
      "source": [
        "#TRPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DpoxxPzPpOV"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_fDObS5PVcR"
      },
      "source": [
        "def to_device(device, *args):\n",
        "  return [x.to(device) for x in args]\n",
        "\n",
        "def get_flat_params_from(model):\n",
        "  params = []\n",
        "  for param in model.parameters():\n",
        "    params.append(param.view(-1))\n",
        "\n",
        "  flat_params = torch.cat(params)\n",
        "  return flat_params\n",
        "\n",
        "\n",
        "def set_flat_params_to(model, flat_params):\n",
        "  prev_ind = 0\n",
        "  for param in model.parameters():\n",
        "    flat_size = int(np.prod(list(param.size())))\n",
        "    param.data.copy_(\n",
        "        flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
        "    prev_ind += flat_size\n",
        "\n",
        "\n",
        "def get_flat_grad_from(inputs, grad_grad=False):\n",
        "  grads = []\n",
        "  for param in inputs:\n",
        "    if grad_grad:\n",
        "        grads.append(param.grad.grad.view(-1))\n",
        "    else:\n",
        "      if param.grad is None:\n",
        "        grads.append(torch.zeros(param.view(-1).shape))\n",
        "      else:\n",
        "        grads.append(param.grad.view(-1))\n",
        "\n",
        "  flat_grad = torch.cat(grads)\n",
        "  return flat_grad\n",
        "\n",
        "\n",
        "def compute_flat_grad(output, inputs, filter_input_ids=set(), retain_graph=False, create_graph=False):\n",
        "    if create_graph:\n",
        "      retain_graph = True\n",
        "\n",
        "    inputs = list(inputs)\n",
        "    params = []\n",
        "    for i, param in enumerate(inputs):\n",
        "      if i not in filter_input_ids:\n",
        "        params.append(param)\n",
        "\n",
        "    grads = torch.autograd.grad(output, params, retain_graph=retain_graph, create_graph=create_graph)\n",
        "\n",
        "    j = 0\n",
        "    out_grads = []\n",
        "    for i, param in enumerate(inputs):\n",
        "      if i in filter_input_ids:\n",
        "        out_grads.append(torch.zeros(param.view(-1).shape, device=param.device, dtype=param.dtype))\n",
        "      else:\n",
        "        out_grads.append(grads[j].view(-1))\n",
        "        j += 1\n",
        "    grads = torch.cat(out_grads)\n",
        "\n",
        "    for param in params:\n",
        "      param.grad = None\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QVvR6xCPwL3"
      },
      "source": [
        "### trpo stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z-P0ckCHH9r"
      },
      "source": [
        "def conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10):\n",
        "  x = torch.zeros(b.size()).to(device)\n",
        "  r = b.clone()\n",
        "  p = b.clone()\n",
        "  rdotr = torch.dot(r, r)\n",
        "  for i in range(nsteps):\n",
        "    _Avp = Avp(p)\n",
        "    alpha = rdotr / torch.dot(p, _Avp)\n",
        "    x += alpha * p\n",
        "    r -= alpha * _Avp\n",
        "    new_rdotr = torch.dot(r, r)\n",
        "    betta = new_rdotr / rdotr\n",
        "    p = r + betta * p\n",
        "    rdotr = new_rdotr\n",
        "    if rdotr < residual_tol:\n",
        "      break\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9JRDr9VHILk"
      },
      "source": [
        "def line_search(model, f, x, fullstep, expected_improve_full, max_backtracks=10, accept_ratio=0.1):\n",
        "  fval = f(True).item()\n",
        "\n",
        "  for stepfrac in [.5**x for x in range(max_backtracks)]:\n",
        "    x_new = x + stepfrac * fullstep\n",
        "    set_flat_params_to(model, x_new)\n",
        "    fval_new = f(True).item()\n",
        "    actual_improve = fval - fval_new\n",
        "    expected_improve = expected_improve_full * stepfrac\n",
        "    ratio = actual_improve / expected_improve\n",
        "\n",
        "    if ratio > accept_ratio:\n",
        "      return True, x_new\n",
        "  return False, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOOtxsLKHUYL"
      },
      "source": [
        "def trpo_step(policy_net, value_net, states, actions, returns, advantages, max_kl, damping, l2_reg, use_fim=True):\n",
        "  #update critic\n",
        "  def get_value_loss(flat_params):\n",
        "    set_flat_params_to(value_net, torch.tensor(flat_params))\n",
        "    for param in value_net.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.fill_(0)\n",
        "    values_pred = value_net(states)\n",
        "    value_loss = (values_pred - returns).pow(2).mean()\n",
        "\n",
        "    # weight decay\n",
        "    for param in value_net.parameters():\n",
        "      value_loss += param.pow(2).sum() * l2_reg\n",
        "    value_loss.backward()\n",
        "    return value_loss.item(), get_flat_grad_from(value_net.parameters()).cpu().numpy()\n",
        "\n",
        "  flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss,\n",
        "                                                          get_flat_params_from(value_net).detach().cpu().numpy(),\n",
        "                                                          maxiter=25)\n",
        "  set_flat_params_to(value_net, torch.tensor(flat_params))\n",
        "\n",
        "  #update policy\n",
        "  with torch.no_grad():\n",
        "    fixed_log_probs = policy_net.get_log_prob(states, actions)\n",
        "  \n",
        "  #define the loss function for TRPO\n",
        "  def get_loss(volatile=False):\n",
        "    with torch.set_grad_enabled(not volatile):\n",
        "      log_probs = policy_net.get_log_prob(states, actions)\n",
        "      action_loss = -advantages * torch.exp(log_probs - fixed_log_probs)\n",
        "      return action_loss.mean()\n",
        "\n",
        "  #use fisher information matrix for Hessian*vector\n",
        "  def Fvp_fim(v):\n",
        "    M, mu = policy_net.get_fim(states)\n",
        "    mu = mu.view(-1)\n",
        "    filter_input_ids = set()\n",
        "\n",
        "    t = torch.ones(mu.size(), requires_grad=True, device=mu.device)\n",
        "    mu_t = (mu * t).sum()\n",
        "    Jt = compute_flat_grad(mu_t, policy_net.parameters(), filter_input_ids=filter_input_ids, create_graph=True)\n",
        "    Jtv = (Jt * v).sum()\n",
        "    Jv = torch.autograd.grad(Jtv, t)[0]\n",
        "    MJv = M * Jv.detach()\n",
        "    mu_MJv = (MJv * mu).sum()\n",
        "    JTMJv = compute_flat_grad(mu_MJv, policy_net.parameters(), filter_input_ids=filter_input_ids).detach()\n",
        "    JTMJv /= states.shape[0]\n",
        "    return JTMJv + v * damping\n",
        "\n",
        "  Fvp = Fvp_fim\n",
        "\n",
        "  loss = get_loss()\n",
        "  grads = torch.autograd.grad(loss, policy_net.parameters())\n",
        "  loss_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n",
        "  stepdir = conjugate_gradients(Fvp, -loss_grad, 10).double()\n",
        "\n",
        "  shs = 0.5 * (stepdir.dot(Fvp(stepdir)))\n",
        "  lm = math.sqrt(max_kl / shs)\n",
        "  fullstep = stepdir * lm\n",
        "  expected_improve = -loss_grad.dot(fullstep)\n",
        "\n",
        "  prev_params = get_flat_params_from(policy_net)\n",
        "  success, new_params = line_search(policy_net, get_loss, prev_params, fullstep, expected_improve)\n",
        "  set_flat_params_to(policy_net, new_params)\n",
        "\n",
        "  return success"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwBcxmGxEgBC"
      },
      "source": [
        "#Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1uN8dIMSgQA"
      },
      "source": [
        "class RunningStat(object):\n",
        "    def __init__(self, shape):\n",
        "        self._n = 0\n",
        "        self._M = np.zeros(shape)\n",
        "        self._S = np.zeros(shape)\n",
        "\n",
        "    def push(self, x):\n",
        "        x = np.asarray(x)\n",
        "        assert x.shape == self._M.shape\n",
        "        self._n += 1\n",
        "        if self._n == 1:\n",
        "            self._M[...] = x\n",
        "        else:\n",
        "            oldM = self._M.copy()\n",
        "            self._M[...] = oldM + (x - oldM) / self._n\n",
        "            self._S[...] = self._S + (x - oldM) * (x - self._M)\n",
        "\n",
        "    @property\n",
        "    def n(self):\n",
        "        return self._n\n",
        "\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return self._M\n",
        "\n",
        "    @property\n",
        "    def var(self):\n",
        "        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n",
        "\n",
        "    @property\n",
        "    def std(self):\n",
        "        return np.sqrt(self.var)\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return self._M.shape\n",
        "\n",
        "class ZFilter:\n",
        "    \"\"\"\n",
        "    y = (x-mean)/std\n",
        "    using running estimates of mean,std\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n",
        "        self.demean = demean\n",
        "        self.destd = destd\n",
        "        self.clip = clip\n",
        "\n",
        "        self.rs = RunningStat(shape)\n",
        "        self.fix = False\n",
        "\n",
        "    def __call__(self, x, update=True):\n",
        "        if update and not self.fix:\n",
        "            self.rs.push(x)\n",
        "        if self.demean:\n",
        "            x = x - self.rs.mean\n",
        "        if self.destd:\n",
        "            x = x / (self.rs.std + 1e-8)\n",
        "        if self.clip:\n",
        "            x = np.clip(x, -self.clip, self.clip)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAaU2oT-Eeiq"
      },
      "source": [
        "def update_params(batch):\n",
        "  states = torch.from_numpy(np.stack(batch.state)).to(device)\n",
        "  actions = torch.from_numpy(np.stack(batch.action)).to(device)\n",
        "  rewards = torch.from_numpy(np.stack(batch.reward)).to(device)\n",
        "  masks = torch.from_numpy(np.stack(batch.mask)).to(device)\n",
        "  with torch.no_grad():\n",
        "    values = value_net(states)\n",
        "\n",
        "  #get advantage estimation from the trajectories\n",
        "  advantages, returns = estimate_advantages(rewards, masks, values, gamma, tau, device)\n",
        "\n",
        "  #perform TRPO update\n",
        "  trpo_step(policy_net, value_net, states, actions, returns, advantages, max_kl, damping, l2_reg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YzwDpDaQVon"
      },
      "source": [
        "def run_training_loop(env, policy_net, value_net, num_episodes=1):\n",
        "  running_state = ZFilter((num_inputs,), clip=5)\n",
        "  running_reward = ZFilter((1,), demean=False, clip=10)\n",
        "\n",
        "  for i_episode in range(num_episodes):\n",
        "    memory = Memory()\n",
        "\n",
        "    num_steps = 0\n",
        "    reward_batch = 0\n",
        "    num_episodes = 0\n",
        "    while num_steps < batch_size:\n",
        "      state = env.reset()\n",
        "      state = np.ndarray.flatten(state)\n",
        "      state = running_state(state)\n",
        "\n",
        "      reward_sum = 0\n",
        "      for t in range(10000): # Don't infinite loop while learning\n",
        "        with torch.no_grad():\n",
        "          action = policy_net.select_action(state)[0].cpu().numpy()\n",
        "        next_state, reward, done, _ = env.step(int(action))\n",
        "        next_state = np.ndarray.flatten(next_state)\n",
        "        reward_sum += reward\n",
        "\n",
        "        next_state = running_state(next_state)\n",
        "\n",
        "        mask = 1\n",
        "        if done:\n",
        "            mask = 0\n",
        "\n",
        "        memory.push(state, int(action), mask, next_state, reward)\n",
        "\n",
        "        if render:\n",
        "          env.render()\n",
        "        if done:\n",
        "          break\n",
        "\n",
        "        state = next_state\n",
        "      num_steps += (t-1)\n",
        "      num_episodes += 1\n",
        "      reward_batch += reward_sum\n",
        "\n",
        "    reward_batch /= num_episodes\n",
        "    batch = memory.sample()\n",
        "    update_params(batch)\n",
        "\n",
        "    if i_episode % log_interval == 0:\n",
        "      print('Episode {}\\tLast reward: {}\\tAverage reward {:.2f}'.format(\n",
        "          i_episode, reward_sum, reward_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aAMvCD-hSulK",
        "outputId": "f7173a62-185f-4a49-9998-e70dfff8b6ed"
      },
      "source": [
        "#@title Demo\n",
        "gamma = 0.995 #@param {type:\"number\"}\n",
        "tau = 0.97 #@param {type:\"number\"}\n",
        "l2_reg = 1e-3 #@param {type:\"number\"}\n",
        "max_kl = 1e-2 #@param {type:\"number\"}\n",
        "damping = 1e-1 #@param {type:\"number\"}\n",
        "batch_size = 15000 #@param {type:\"slider\", min:0, max:20000, step:100}\n",
        "num_episodes = 100 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "log_interval = 1 #@param {type:\"number\"}\n",
        "env_name = \"procgen:procgen-coinrun-v0\" #@param {type:\"string\"}\n",
        "render = False #@param {type: \"boolean\"}\n",
        "\n",
        "seed = 4\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "num_inputs = np.prod(env.observation_space.shape)\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "policy_net = Policy(num_inputs, num_actions).double().cuda()\n",
        "value_net = Value(num_inputs).double().cuda()\n",
        "\n",
        "run_training_loop(env, policy_net, value_net, num_episodes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0\tLast reward: 0.0\tAverage reward 0.83\n",
            "Episode 1\tLast reward: 0.0\tAverage reward 3.33\n",
            "Episode 2\tLast reward: 10.0\tAverage reward 3.33\n",
            "Episode 3\tLast reward: 0.0\tAverage reward 3.53\n",
            "Episode 4\tLast reward: 0.0\tAverage reward 4.17\n",
            "Episode 5\tLast reward: 0.0\tAverage reward 4.05\n",
            "Episode 6\tLast reward: 0.0\tAverage reward 5.32\n",
            "Episode 7\tLast reward: 0.0\tAverage reward 3.47\n",
            "Episode 8\tLast reward: 0.0\tAverage reward 3.93\n",
            "Episode 9\tLast reward: 0.0\tAverage reward 4.65\n",
            "Episode 10\tLast reward: 0.0\tAverage reward 5.00\n",
            "Episode 11\tLast reward: 10.0\tAverage reward 4.00\n",
            "Episode 12\tLast reward: 10.0\tAverage reward 5.36\n",
            "Episode 13\tLast reward: 0.0\tAverage reward 3.88\n",
            "Episode 14\tLast reward: 0.0\tAverage reward 3.61\n",
            "Episode 15\tLast reward: 0.0\tAverage reward 3.94\n",
            "Episode 16\tLast reward: 0.0\tAverage reward 4.35\n",
            "Episode 17\tLast reward: 0.0\tAverage reward 4.70\n",
            "Episode 18\tLast reward: 0.0\tAverage reward 4.53\n",
            "Episode 19\tLast reward: 0.0\tAverage reward 5.14\n",
            "Episode 20\tLast reward: 0.0\tAverage reward 3.54\n",
            "Episode 21\tLast reward: 10.0\tAverage reward 5.00\n",
            "Episode 22\tLast reward: 0.0\tAverage reward 3.77\n",
            "Episode 23\tLast reward: 0.0\tAverage reward 4.85\n",
            "Episode 24\tLast reward: 0.0\tAverage reward 5.10\n",
            "Episode 25\tLast reward: 0.0\tAverage reward 5.39\n",
            "Episode 26\tLast reward: 0.0\tAverage reward 3.68\n",
            "Episode 27\tLast reward: 10.0\tAverage reward 5.00\n",
            "Episode 28\tLast reward: 10.0\tAverage reward 4.68\n",
            "Episode 29\tLast reward: 10.0\tAverage reward 4.86\n",
            "Episode 30\tLast reward: 0.0\tAverage reward 4.65\n",
            "Episode 31\tLast reward: 0.0\tAverage reward 4.30\n",
            "Episode 32\tLast reward: 10.0\tAverage reward 4.45\n",
            "Episode 33\tLast reward: 10.0\tAverage reward 5.19\n",
            "Episode 34\tLast reward: 0.0\tAverage reward 4.83\n",
            "Episode 35\tLast reward: 0.0\tAverage reward 4.76\n",
            "Episode 36\tLast reward: 0.0\tAverage reward 5.15\n",
            "Episode 37\tLast reward: 10.0\tAverage reward 5.45\n",
            "Episode 38\tLast reward: 0.0\tAverage reward 5.00\n",
            "Episode 39\tLast reward: 0.0\tAverage reward 4.96\n",
            "Episode 40\tLast reward: 0.0\tAverage reward 5.22\n",
            "Episode 41\tLast reward: 10.0\tAverage reward 4.55\n",
            "Episode 42\tLast reward: 10.0\tAverage reward 5.37\n",
            "Episode 43\tLast reward: 10.0\tAverage reward 4.97\n",
            "Episode 44\tLast reward: 0.0\tAverage reward 4.87\n",
            "Episode 45\tLast reward: 10.0\tAverage reward 4.68\n",
            "Episode 46\tLast reward: 10.0\tAverage reward 4.60\n",
            "Episode 47\tLast reward: 10.0\tAverage reward 5.06\n",
            "Episode 48\tLast reward: 0.0\tAverage reward 5.19\n",
            "Episode 49\tLast reward: 0.0\tAverage reward 4.97\n",
            "Episode 50\tLast reward: 10.0\tAverage reward 5.03\n",
            "Episode 51\tLast reward: 0.0\tAverage reward 4.53\n",
            "Episode 52\tLast reward: 0.0\tAverage reward 4.61\n",
            "Episode 53\tLast reward: 10.0\tAverage reward 4.85\n",
            "Episode 54\tLast reward: 10.0\tAverage reward 5.44\n",
            "Episode 55\tLast reward: 0.0\tAverage reward 4.47\n",
            "Episode 56\tLast reward: 0.0\tAverage reward 4.70\n",
            "Episode 57\tLast reward: 0.0\tAverage reward 5.37\n",
            "Episode 58\tLast reward: 0.0\tAverage reward 4.68\n",
            "Episode 59\tLast reward: 0.0\tAverage reward 4.80\n",
            "Episode 60\tLast reward: 10.0\tAverage reward 5.18\n",
            "Episode 61\tLast reward: 0.0\tAverage reward 5.08\n",
            "Episode 62\tLast reward: 10.0\tAverage reward 5.21\n",
            "Episode 63\tLast reward: 0.0\tAverage reward 5.18\n",
            "Episode 64\tLast reward: 10.0\tAverage reward 4.64\n",
            "Episode 65\tLast reward: 0.0\tAverage reward 5.46\n",
            "Episode 66\tLast reward: 10.0\tAverage reward 4.86\n",
            "Episode 67\tLast reward: 0.0\tAverage reward 5.37\n",
            "Episode 68\tLast reward: 0.0\tAverage reward 5.19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-36b914932c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mvalue_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mrun_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-57c205559c22>\u001b[0m in \u001b[0;36mrun_training_loop\u001b[0;34m(env, policy_net, value_net, num_episodes)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mreward_batch\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-d03a78d218ab>\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#get advantage estimation from the trajectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_advantages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m#perform TRPO update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-a39f266a4b29>\u001b[0m in \u001b[0;36mestimate_advantages\u001b[0;34m(rewards, masks, values, gamma, tau, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_value\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0madvantages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_advantage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprev_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}